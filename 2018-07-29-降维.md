降维

> 参考文献：
>
> 1. [简述多种降维算法](https://chenrudan.github.io/blog/2016/04/01/dimensionalityreduction.html)
> 2. 【matlab 在数学建模中的应用】
> 3. [Python 机器学习](http://scikit-learn.org/stable/modules/decomposition.html#decompositions)



### 模型的原理——建模

#### 原理部分

在[机器学习](https://zh.wikipedia.org/wiki/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0)和[统计学](https://zh.wikipedia.org/wiki/%E7%BB%9F%E8%AE%A1%E5%AD%A6)领域，**降维**是指在某些限定条件下，降低随机变量个数，得到一组**“不相关”主变量**的过程。 降维可进一步细分为[特征选择](https://zh.wikipedia.org/wiki/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9)和[特征提取](https://zh.wikipedia.org/wiki/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96)两大方法。这两个是不一样的：

1. [特征选择](https://zh.wikipedia.org/wiki/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9)假定数据中包含大量冗余或无关变量（或称特征、属性、指标等），旨在从原有变量中**找出主要变量** ,从而只是找出而未加变换。其代表方法为[LASSO](https://zh.wikipedia.org/wiki/Lasso%E7%AE%97%E6%B3%95)。
2. [特征提取](https://zh.wikipedia.org/wiki/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96)是将高维数据转化为低维数据的过程。在此过程中可能**舍弃原有数据、创造新的变量**，其代表方法为[主成分分析](https://zh.wikipedia.org/wiki/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90)。



**信息算法表：**

![img](http://7xkmdr.com1.z0.glb.clouddn.com/reduction.png)

> 1. 所谓去中心化即每个数据减去对应列的均值，可以联想原点矩与中心矩的定义
> 2. 参数说明：
>    * $X_{D\times N}$ 高维输入矩阵，其中 $D$ 为高维数（即变量/指标数），$N$ 为样本个数
>    * $C=XX^T$
>    * $Z_{d\times N}$ 表示降维输出矩阵
>    * $E=ZZ^T$ ，线性映射是 $Z=W^TX$ 
>    * $A$ ：高维空间中样本点俩俩之间的距离
>    * $S_w,S_b$ 分别是LDA的类内散度矩阵和类间散度矩阵
>    * $k$ 表示流形学习中一个点与 $k$ 个点是邻近关系
>    * $F$ 表示高维空间中一个点由周围几个点的线性组合矩阵
>    * $M=(I-F)(I-F)^T$
>    * $P$ 是高维空间中两点距离占所有距离比重的概率矩阵
>    * $Q$ 是低维空间中两点距离占所有距离比重的概率矩阵
>    * $l$ 表示全连接神经网络的层数，$D_l$ 表示每一层的节点个数



##### 为何要降维？

降维的意思是能够用一组个数为 $d$ 的向量 $z_i$ 来代表个数为 $D$ 的向量 $x_i$ 所包含的有用信息，其中 $d<D$ 。而高维空间的数据很有可能出现分布**稀疏**的情况，即100个样本在100维空间分布肯定是非常稀疏的，每增加一维所需的样本个数呈**指数级增长**，这种在高维空间中样本稀疏的问题被称为维数灾难。降维可以缓解这种问题。

##### 从什么角度出发来降维？

1. 直接提取特征子集做特征抽取

2. 通过线性/非线性的方式将原来高维空间**变换**到一个新的空间，又主要有两种

   * 一种是基于从高维空间映射到低维空间的projection方法

     > 主要目的就是学习或者算出一个矩阵变换 $W$，用这个矩阵与高维数据相乘得到低维数据: 代表有
     >
     > * PCA
     > * LDA
     > * Auto encoder


   * 一种是基于流形学习的方法，流形学习的目的是找到**高维空间样本的低维描述**

     > 假设在高维空间中数据会呈现一种有规律的**低维流形排列**，但是这种规律排列**不能**直接**通过**高维空间的**欧式距离**来衡量。
     >
     > ![img](https://pic1.zhimg.com/80/8d3cf3947b5497f00075bd17d5dc5e4c_hd.jpg)



#### 主成分分析 PCA

目标：降维后低维样本之间每一维**方差**尽可能大，从而便于**区分**

> 方差反映了数据差异的程度

设 $Z_i (i=1,2,\cdots,d)$ 表示第 $i$ 个主成分，可设
$$
Z_i=c_{i1}X_1+c_{i2}X_2+\cdots+c_{iN}X_N
$$
且 $T_i=c_{i1}^2+c_{i2}^2+\cdots+c_{iN}^2=1$ ，这些系数为互相垂直的单位向量并且使得这一维度的方差最大，即 $<T_i,T_{i-1}>=0$ 且
$$
T_i=\arg \max Var(Z_i)
$$
这样以后便只需要确定主成分的个数，当然要以**信息量减少最少**为标准

> 1. 主成分分析的结果受量纲的影响，这是主成分分析的最大问题，**回归分析是不存在这种情况的**，所以实际中可以先把各变量的数据**标准化**，然后使用协方差矩阵或相关系数矩阵进行分析
>
>    > 什么时候不需要标准化
>    >
>    > 1. 当采用普通的**线性回归**的时候，是无需标准化的。因为标准化前后，不会影响线性回归**预测值**。
>    > 2. 同时，标准化不会影响**logistic**回归，决策树及其他一些**集成学习**算法：such as random forest and gradient boosting.
>
> 2. 使方差达到最大的主成分分析不用转轴（由于统计软件常把主成分分析和因子分析放在一起，后者往往需要转轴，使用时应注意）。
>
> 3. 主成分的保留。用**相关系数矩阵求主成分**时，Kaiser主张将特征值小于1的主成分予以放弃（这也是SPSS软件的默认值）。
>
> 4. **主成分回归**：为了克服变量多重共线性，采用变换之后地主成分进行最小二乘估计，估计完后再变换回来



##### **特征值因子的筛选（主成分的确定）：**

计设计阵为 $X_{D\times N}$ ，实际上确定 $c_{ij}$ 即确定 $X^TX$ 的特征向量。于是只需要将特征值按大小排列进行筛选

> 　特征值＞１表明是伸长变换，从而增大方差

一般实用的删选方法是
$$
\frac{\sum_{i=1}^d\lambda_i}{\sum_{i=1}^D\lambda_i}\ge85\%
$$
而使用 Z-score 进行标准化之后($\bar{X}$) ，由于变量的相关系数矩阵
$$
R=\frac{\bar{X}^T\bar{X}}{N}
$$
从而主成分分析只取相关系数的特征值和特征向量即可。有时还需要考虑选择的主成分对原始变量的贡献值，我们用相关系数的平方和来表示，
$$
\rho_i=\sum_{j=1}^d r^2(Z_i,X_i)
$$

降维体现在哪呢？经过特征值分解，可以得到
$$
R=\frac{\bar{X}^T\bar{X}}{N-1}=U \Lambda U^T=\sum_a \lambda_au_au^T_a \\
\Lambda=diag(\lambda_1,\lambda_2,\cdots,\lambda_D)
$$
取前 $d$ 个特征向量，则将一个 $D$ 维的 $X_i,(i=1,2,\cdots,N)$ 向 $d$ 维的主成分方向（特征向量方向）**投影后的降维数据**：
$$
Y_i=U_k^TX_i
$$

> 1. 我们知道 $A$ 在$B$ 的投影即：$||A||cos(\theta)$  ，而**协方差的特征向量是单位正交的。**
> 2. 可视单化方法：https://terrytangyuan.github.io/2015/11/24/ggfortify-intro/



**PCA 流程图：**

![](https://i.loli.net/2018/07/30/5b5ea072d2aa5.png)



##### PCA 变种——Kernel PCA

首先PCA是一个线性模型，想要处理非线性问题便可以采取**核**手段。

![../../_images/sphx_glr_plot_kernel_pca_001.png](http://scikit-learn.org/stable/_images/sphx_glr_plot_kernel_pca_001.png)

KPCA 首先将数据映射到更高维度空间，成为特征空间（Hibert泛函空间），再在这个空间上做PCA

定义映射 $\Phi :X^D \rightarrow F^p$ ，$F$ 可以是无穷维的 $p>D$。设经过映射后且经过中心化（均值为零）的数据为 $\Phi(X_i)$ ，则其协方差矩阵
$$
\bar{C}_{ij}=E[\Phi(X_i)\cdot\Phi(X_j)]-E[\Phi(X_i)]\cdot E[\Phi(X_j)]\\
\bar{C}=\frac{\Phi(X)\Phi(X_i)^T}{N}
$$

> 关键在与如何求解，参考文献说上述协方差矩阵无法得到。。。可是给定一个核函数不就得到了吗，然后特征分解,投影就好了呀。。那篇文章给了 MATLAB 实例代码，我放在 **/code/降维/** 目录下，好像就是直接分解的。编程时整理下这份代码**和**python 代码（之前发的[python机器学习网站](http://scikit-learn.org/stable/auto_examples/decomposition/plot_kernel_pca.html#sphx-glr-auto-examples-decomposition-plot-kernel-pca-py)）



#### 因子分析 FA

设 $D$ 个变量 $X_i,i=1,2,\cdots,D$  可以表示为
$$
X_i=\mu_i+\alpha_{i1}F_1+\alpha_{i1}F_2+\cdots,+\alpha_{id}F_d+\varepsilon_i,d<D  
$$
其中 $\mu$ 为偏移向量， $F_k$ 称为公共因子，为不可观测变量。其系数称为载荷因子，$\varepsilon \sim N(0,\Phi)$ 为正态噪声，$D(\varepsilon)=diag(\sigma_1^2,\sigma_2^2,\cdots,\sigma_D^2)$（特殊因子的方差）。



因子载荷矩阵 $\Lambda =(\alpha_{ij})_{D \times d}$ 的性质：

**一、因子载荷的统计意义：**

$\alpha_{ij}$ 是变量与公共因子之间的相关系数。绝对值越大，相关程度越大

**二、变量共同度**：第 $i$ 行的元素平凡和
$$
h_i^2=\sum_{j=1}^d\alpha_{ij}^2
$$
由因子模型左右求方差可得 
$$
1=h_i^2+\sigma_i^2
$$
从而变量共同度越接近于与1越好。

三、公共因子 $F_j$ 方差贡献
$$
S_j=\sum_{i=1}^D\alpha_{ij}^2
$$

##### 载荷矩阵估计方法

###### 主成分分析法

样本相关系数矩阵
$$
R=\frac{\bar{X}^T\bar{X}}{N}
$$
$\lambda_k,u_k$ 分别为特征值与特征向量。其中 $u_k$ 需经过标准正交化

> 但是样本标准化之后的相关矩阵的特征向量应该说是单位正交的，编程时记得检查验证一下。

载荷矩阵估计为：
$$
\Lambda=[\sqrt{\lambda_1}u_1,\sqrt{\lambda_2}u_2,\cdots,\sqrt{\lambda_d}u_d ]
$$
特殊因子的方差估计
$$
\sigma_i^2=1-\sum_{j=1}^d\alpha_{ij}^2
$$

###### 主因子法

$R^*=R-D(\varepsilon)$，实际情况通常不知道特殊方差（上一中方法尽然用未知估计未知。。），可以采取如下估计：$\widehat{h}_i^2=\max |r_{ij}|,i \ne j$

再对 $R^*$ 分解估计载荷矩阵

###### 最大似然估计

matlab默认估计载荷矩阵方法，函数为 `factoran`

##### 因子旋转

使载荷矩阵结构简化

1. 方差最大法；
2. 四次方最大旋转
3. 等量最大法

#####  因子得分——估计公共因子

因子得分函数：
$$
F_j=c_j+\beta_{j1}X_1+\cdots+\beta_{jD}X_D,j=1,2,\cdots,d
$$
问题等价于：
$$
\min \quad(X-\mu-\Lambda F)^TD^{-1}(X-\mu-\Lambda F) \\
s.t. \quad X-\mu=\Lambda F+\varepsilon
$$
求得最优估计 $F=(\Lambda^TD^{-1}\Lambda)^{-1}\Lambda^TD^{-1}(X-\mu)$



#### 判别分析

LDA最开始是作为解决**二分类**问题由Fisher在1936年提出，由于**计算过程实际上对数据做了降维处理**，因此也可用作监督线性降维。它通过将高维空间数据投影到低维空间，在**低维**空间中确定每个样本所属的**类**，这里考虑 $K$ 个类的情况。它的目标是将样本能尽可能正确的分成 $K$ 类，体现为**同类样本投影点尽可能近，不同类样本点尽可能远**，这点跟PCA就不一样、PCA是希望所有样本在某一个维数上尽可能分开，LDA的低维投影**可能会重叠**，但是PCA就不希望投影点重叠。它采用的降维思路跟PCA是一样的，都是通过矩阵乘法来进行线性降维，投影点是
$$
Z_i=W^T\cdot X_i
$$
![1](http://7xkmdr.com1.z0.glb.clouddn.com/lda.jpg)

由于希望属于不同类的样本尽可能离的远，那么就希望投影过后的**投影中心点**离的尽可能远，容易想到的目标是
$$
W=\arg \max ||W^T\mu_1-W^T\mu_2||^2
$$
但是这还不够，上图中直接投下来距离就足够大，但是出现了重叠。

