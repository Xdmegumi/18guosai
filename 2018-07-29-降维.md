降维

> 参考文献：
>
> 1. [简述多种降维算法](https://chenrudan.github.io/blog/2016/04/01/dimensionalityreduction.html)
> 2. 【matlab 在数学建模中的应用】
> 3. [Python 机器学习](http://scikit-learn.org/stable/modules/decomposition.html#decompositions)



### 模型的原理——建模

#### 原理部分

在[机器学习](https://zh.wikipedia.org/wiki/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0)和[统计学](https://zh.wikipedia.org/wiki/%E7%BB%9F%E8%AE%A1%E5%AD%A6)领域，**降维**是指在某些限定条件下，降低随机变量个数，得到一组**“不相关”主变量**的过程。 降维可进一步细分为[特征选择](https://zh.wikipedia.org/wiki/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9)和[特征提取](https://zh.wikipedia.org/wiki/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96)两大方法。这两个是不一样的：

1. [特征选择](https://zh.wikipedia.org/wiki/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9)假定数据中包含大量冗余或无关变量（或称特征、属性、指标等），旨在从原有变量中**找出主要变量** ,从而只是找出而未加变换。其代表方法为[LASSO](https://zh.wikipedia.org/wiki/Lasso%E7%AE%97%E6%B3%95)。
2. [特征提取](https://zh.wikipedia.org/wiki/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96)是将高维数据转化为低维数据的过程。在此过程中可能**舍弃原有数据、创造新的变量**，其代表方法为[主成分分析](https://zh.wikipedia.org/wiki/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90)。



**信息算法表：**

![img](http://7xkmdr.com1.z0.glb.clouddn.com/reduction.png)

> 1. 所谓去中心化即每个数据减去对应列的均值，可以联想原点矩与中心矩的定义
> 2. 参数说明：
>    * $X_{D\times N}$ 高维输入矩阵，其中 $D$ 为高维数（即变量/指标数），$N$ 为样本个数
>    * $C=XX^T$
>    * $Z_{d\times N}$ 表示降维输出矩阵
>    * $E=ZZ^T$ ，线性映射是 $Z=W^TX$ 
>    * $A$ ：高维空间中样本点俩俩之间的距离
>    * $S_w,S_b$ 分别是LDA的类内散度矩阵和类间散度矩阵
>    * $k$ 表示流形学习中一个点与 $k$ 个点是邻近关系
>    * $F$ 表示高维空间中一个点由周围几个点的线性组合矩阵
>    * $M=(I-F)(I-F)^T$
>    * $P$ 是高维空间中两点距离占所有距离比重的概率矩阵
>    * $Q$ 是低维空间中两点距离占所有距离比重的概率矩阵
>    * $l$ 表示全连接神经网络的层数，$D_l$ 表示每一层的节点个数



##### 为何要降维？

降维的意思是能够用一组个数为 $d$ 的向量 $z_i$ 来代表个数为 $D$ 的向量 $x_i$ 所包含的有用信息，其中 $d<D$ 。而高维空间的数据很有可能出现分布**稀疏**的情况，即100个样本在100维空间分布肯定是非常稀疏的，每增加一维所需的样本个数呈**指数级增长**，这种在高维空间中样本稀疏的问题被称为维数灾难。降维可以缓解这种问题。

##### 从什么角度出发来降维？

1. 直接提取特征子集做特征抽取

2. 通过线性/非线性的方式将原来高维空间**变换**到一个新的空间，又主要有两种

   * 一种是基于从高维空间映射到低维空间的projection方法

     > 主要目的就是学习或者算出一个矩阵变换 $W$，用这个矩阵与高维数据相乘得到低维数据: 代表有
     >
     > * PCA
     > * LDA
     > * Auto encoder


   * 一种是基于流形学习的方法，流形学习的目的是找到**高维空间样本的低维描述**

     > 假设在高维空间中数据会呈现一种有规律的**低维流形排列**，但是这种规律排列**不能**直接**通过**高维空间的**欧式距离**来衡量。
     >
     > ![img](https://pic1.zhimg.com/80/8d3cf3947b5497f00075bd17d5dc5e4c_hd.jpg)



#### 主成分分析 PCA

目标：降维后低维样本之间每一维**方差**尽可能大，从而便于**区分**

> 方差反映了数据差异的程度

设 $Z_i (i=1,2,\cdots,d)$ 表示第 $i$ 个主成分，可设
$$
Z_i=c_{i1}X_1+c_{i2}X_2+\cdots+c_{iN}X_N
$$
且 $T_i=c_{i1}^2+c_{i2}^2+\cdots+c_{iN}^2=1$ ，这些系数为互相垂直的单位向量并且使得这一维度的方差最大，即 $<T_i,T_{i-1}>=0$ 且
$$
T_i=\arg \max Var(Z_i)
$$
这样以后便只需要确定主成分的个数，当然要以**信息量减少最少**为标准

> 1. 主成分分析的结果受量纲的影响，这是主成分分析的最大问题，**回归分析是不存在这种情况的**，所以实际中可以先把各变量的数据**标准化**，然后使用协方差矩阵或相关系数矩阵进行分析
>
>    > 什么时候不需要标准化
>    >
>    > 1. 当采用普通的**线性回归**的时候，是无需标准化的。因为标准化前后，不会影响线性回归**预测值**。
>    > 2. 同时，标准化不会影响**logistic**回归，决策树及其他一些**集成学习**算法：such as random forest and gradient boosting.
>
> 2. 使方差达到最大的主成分分析不用转轴（由于统计软件常把主成分分析和因子分析放在一起，后者往往需要转轴，使用时应注意）。
>
> 3. 主成分的保留。用**相关系数矩阵求主成分**时，Kaiser主张将特征值小于1的主成分予以放弃（这也是SPSS软件的默认值）。
>
> 4. **主成分回归**：为了克服变量多重共线性，采用变换之后地主成分进行最小二乘估计，估计完后再变换回来



##### **特征值因子的筛选（主成分的确定）：**

计设计阵为 $X_{D\times N}$ ，实际上确定 $c_{ij}$ 即确定 $X^TX$ 的特征向量。于是只需要将特征值按大小排列进行筛选

> 　特征值＞１表明是伸长变换，从而增大方差

一般实用的删选方法是
$$
\frac{\sum_{i=1}^d\lambda_i}{\sum_{i=1}^D\lambda_i}\ge85\%
$$
而使用 Z-score 进行标准化之后($\bar{X}$) ，由于变量的相关系数矩阵
$$
R=\frac{\bar{X}^T\bar{X}}{N-1}
$$
从而主成分分析只取相关系数的特征值和特征向量即可。有时还需要考虑选择的主成分对原始变量的贡献值，我们用相关系数的平方和来表示，
$$
\rho_i=\sum_{j=1}^d r^2(Z_i,X_i)
$$

> 可视化方法：https://terrytangyuan.github.io/2015/11/24/ggfortify-intro/

##### PCA 变种

