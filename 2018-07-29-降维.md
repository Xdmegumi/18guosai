降维

> 参考文献：
>
> 1. [简述多种降维算法](https://chenrudan.github.io/blog/2016/04/01/dimensionalityreduction.html)
> 2. 【matlab 在数学建模中的应用】
> 3. [Python 机器学习](http://scikit-learn.org/stable/modules/decomposition.html#decompositions)



### 模型的原理——建模

#### 原理部分

在[机器学习](https://zh.wikipedia.org/wiki/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0)和[统计学](https://zh.wikipedia.org/wiki/%E7%BB%9F%E8%AE%A1%E5%AD%A6)领域，**降维**是指在某些限定条件下，降低随机变量个数，得到一组**“不相关”主变量**的过程。 降维可进一步细分为[特征选择](https://zh.wikipedia.org/wiki/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9)和[特征提取](https://zh.wikipedia.org/wiki/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96)两大方法。这两个是不一样的：

1. [特征选择](https://zh.wikipedia.org/wiki/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9)假定数据中包含大量冗余或无关变量（或称特征、属性、指标等），旨在从原有变量中**找出主要变量** ,从而只是找出而未加变换。其代表方法为[LASSO](https://zh.wikipedia.org/wiki/Lasso%E7%AE%97%E6%B3%95)。
2. [特征提取](https://zh.wikipedia.org/wiki/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96)是将高维数据转化为低维数据的过程。在此过程中可能**舍弃原有数据、创造新的变量**，其代表方法为[主成分分析](https://zh.wikipedia.org/wiki/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90)。



**信息算法表：**

![img](http://7xkmdr.com1.z0.glb.clouddn.com/reduction.png)

> 1. 上述总结有误，MDS为一种线性降维。
>
> 2. 所谓去中心化即每个数据减去对应列的均值，可以联想原点矩与中心矩的定义
>
> 3. 参数说明：
>    * $X_{D\times N}$ 高维输入矩阵，其中 $D$ 为高维数（即变量/指标数），$N$ 为样本个数
>    * $C=XX^T$
>    * $Z_{d\times N}$ 表示降维输出矩阵
>    * $E=ZZ^T$ ，线性映射是 $Z=W^TX$ 
>    * $A$ ：高维空间中样本点俩俩之间的距离
>    * $S_w,S_b$ 分别 是LDA 的类内散度矩阵和类间散度矩阵
>    * $k$ 表示流形学习中一个点与 $k$ 个点是邻近关系
>    * $F$ 表示高维空间中一个点由周围几个点的线性组合矩阵
>    * $M=(I-F)(I-F)^T$
>    * $P$ 是高维空间中两点距离占所有距离比重的概率矩阵
>    * $Q$ 是低维空间中两点距离占所有距离比重的概率矩阵
>
>    $l$ 表示全连接神经网络的层数，$D_l$ 表示每一层的节点个数



但凡涉及**矩阵或者向量**运算，都要用**粗体数学环境**，随机变量不用



##### 为何要降维？

降维的意思是能够用一组个数为 $d$ 的向量 $z_i$ 来代表个数为 $D$ 的向量 $x_i$ 所包含的有用信息，其中 $d<D$ 。而高维空间的数据很有可能出现分布**稀疏**的情况，即100个样本在100维空间分布肯定是非常稀疏的，每增加一维所需的样本个数呈**指数级增长**，这种在高维空间中样本稀疏的问题被称为维数灾难。降维可以缓解这种问题。

##### 从什么角度出发来降维？

1. 直接提取特征子集做特征抽取

2. 通过线性/非线性的方式将原来高维空间**变换**到一个新的空间，又主要有两种

   * 一种是基于从高维空间映射到低维空间的projection方法

     > 主要目的就是学习或者算出一个矩阵变换 $W$，用这个矩阵与高维数据相乘得到低维数据: 代表有
     >
     > * PCA
     > * LDA
     > * Auto encoder


   * 一种是基于流形学习的方法，流形学习的目的是找到**高维空间样本的低维描述**

     > 假设在高维空间中数据会呈现一种有规律的**低维流形排列**，但是这种规律排列**不能**直接**通过**高维空间的**欧式距离**来衡量。
     >
     > ![img](https://pic1.zhimg.com/80/8d3cf3947b5497f00075bd17d5dc5e4c_hd.jpg)



#### 主成分分析 PCA

目标：降维后低维样本之间每一维**方差**尽可能大，从而便于**区分**

> 方差反映了数据差异的程度

设 $Z_i (i=1,2,\cdots,D)$ 表示第 $i$ 个主成分，可设
$$
Z_i=c_{i1}X_1+c_{i2}X_2+\cdots+c_{iD}X_D
$$
且 $T_i=c_{i1}^2+c_{i2}^2+\cdots+c_{iD}^2=1$ ，这些系数为互相垂直的单位向量并且使得这一维度的方差最大，即 $<T_i,T_{i-1}>=0$ 且
$$
T_i=\arg \max Var(Z_i)
$$
这样以后便只需要确定主成分的个数，当然要以**信息量减少最少**为标准

> 1. 主成分分析的结果受量纲的影响，这是主成分分析的最大问题，**回归分析是不存在这种情况的**，所以实际中可以先把各变量的数据**标准化**，然后使用协方差矩阵或相关系数矩阵进行分析
>
>    > 什么时候不需要标准化
>    >
>    > 1. 当采用普通的**线性回归**的时候，是无需标准化的。因为标准化前后，不会影响线性回归**预测值**。
>    > 2. 同时，标准化不会影响**logistic**回归，决策树及其他一些**集成学习**算法：such as random forest and gradient boosting.
>
> 2. 使方差达到最大的主成分分析不用转轴（由于统计软件常把主成分分析和因子分析放在一起，后者往往需要转轴，使用时应注意）。
>
> 3. 主成分的保留。用**相关系数矩阵求主成分**时，Kaiser主张将特征值小于1的主成分予以放弃（这也是SPSS软件的默认值）。
>
> 4. **主成分回归**：为了克服变量多重共线性，采用变换之后地主成分进行最小二乘估计，估计完后再变换回来



##### **特征值因子的筛选（主成分的确定）：**

计设计阵为 $X_{D\times N}$ ，实际上确定 $c_{ij}$ 即确定 $\boldsymbol{X}\boldsymbol{X}^T$ 的特征向量。于是只需要将特征值按大小排列进行筛选

> 　特征值＞１表明是伸长变换，从而增大方差

一般实用的删选方法是
$$
\frac{\sum_{i=1}^d\lambda_i}{\sum_{i=1}^D\lambda_i}\ge85\%
$$
而使用 Z-score 进行标准化之后($\bar{\boldsymbol{X}}$) ，由于**样本**变量的相关系数矩阵
$$
(\boldsymbol{R})_{D\times D}=\frac{\boldsymbol{X}\boldsymbol{X}^T}{N-1} \\
R_{jk}=\frac{1}{N-1}\sum_{i=1}^NX_{ji}X_{ki}
$$
从而主成分分析只取相关系数的特征值和特征向量即可。有时还需要考虑选择的主成分对原始变量的贡献值，我们用相关系数的平方和来表示，
$$
\rho_i=\sum_{j=1}^d r^2(Z_j,X_i)
$$

降维体现在哪呢？经过特征值分解，可以得到
$$
\boldsymbol{R}=\frac{\boldsymbol{X}\boldsymbol{X}^T}{N-1}=\boldsymbol{U}\boldsymbol{\Lambda} \boldsymbol{U}^T=\sum_a \lambda_au_au^T_a \\
\boldsymbol{\Lambda} =diag(\lambda_1,\lambda_2,\cdots,\lambda_D)
$$
取前 $d$ 个特征向量，则将一个 $D$ 维的 $X_i,(i=1,2,\cdots,N)$ 向 $d$ 维的主成分方向（特征向量方向）**投影后的降维数据**：
$$
Y_i=\boldsymbol{U_k^T} X_i
$$

> 1. 我们知道 $A$ 在$B$ 的投影即：$||A||cos(\theta)$  ，而**协方差的特征向量是单位正交的。**
> 2. $\frac{\boldsymbol{X}\boldsymbol{X}^T}{N-1}$ 和 $\boldsymbol{X}\boldsymbol{X}^T$ 的特征向量是一样的，只是特征值差了一个**倍数**，不影响降维
> 3. 可视化方法：https://terrytangyuan.github.io/2015/11/24/ggfortify-intro/



**PCA 流程图：**

![](https://i.loli.net/2018/07/30/5b5ea072d2aa5.png)



##### PCA 变种——Kernel PCA

首先PCA是一个线性模型，想要处理非线性问题便可以采取**核**手段。

![../../_images/sphx_glr_plot_kernel_pca_001.png](http://scikit-learn.org/stable/_images/sphx_glr_plot_kernel_pca_001.png)

KPCA 首先将数据映射到更高维度空间，成为特征空间（Hibert泛函空间），再在这个空间上做PCA

定义映射 $\Phi :X^D \rightarrow F^p$ ，$F$ 可以是无穷维的 $p>D$。设经过映射后且经过中心化（均值为零）的数据为 $\Phi(X_i)$ ，则其协方差矩阵
$$
\bar{C}_{ij}=E[\Phi(X_i)\cdot\Phi(X_j)]-E[\Phi(X_i)]\cdot E[\Phi(X_j)]\\
\boldsymbol{\bar{C}}=\frac{\Phi(\boldsymbol{X})\Phi(\boldsymbol{X})^T}{N}
$$

> 关键在与如何求解，参考文献说上述协方差矩阵无法得到。。。可是给定一个核函数不就可以估计样本协方差吗，然后特征分解,投影就好了呀。。那篇文章给了 MATLAB 实例代码，我放在 **/code/降维/** 目录下，好像就是直接分解的。编程时整理下这份代码**和**python 代码（之前发的[python机器学习网站](http://scikit-learn.org/stable/auto_examples/decomposition/plot_kernel_pca.html#sphx-glr-auto-examples-decomposition-plot-kernel-pca-py)）



#### 因子分析 FA

设 $D$ 个变量 $X_i,i=1,2,\cdots,D$  可以表示为
$$
X_i=\mu_i+\alpha_{i1}F_1+\alpha_{i1}F_2+\cdots,+\alpha_{id}F_d+\varepsilon_i,d<D  
$$
其中 $\mu$ 为偏移向量， $F_k$ 称为公共因子，为不可观测变量。其系数称为载荷因子，$\varepsilon \sim N(0,\Phi)$ 为正态噪声，$\boldsymbol{D}(\varepsilon)=diag(\sigma_1^2,\sigma_2^2,\cdots,\sigma_D^2)$（特殊因子的方差）。



因子载荷矩阵 $\boldsymbol{\Lambda} =(\alpha_{ij})_{D \times d}$ 的性质：

**一、因子载荷的统计意义：**

$\alpha_{ij}$ 是变量与公共因子之间的相关系数。绝对值越大，相关程度越大

**二、变量共同度**：第 $i$ 行的元素平凡和
$$
h_i^2=\sum_{j=1}^d\alpha_{ij}^2
$$
由因子模型左右求方差可得 
$$
1=h_i^2+\sigma_i^2
$$
从而变量共同度越接近于与1越好。

三、公共因子 $F_j$ 方差贡献
$$
S_j=\sum_{i=1}^D\alpha_{ij}^2
$$

##### 载荷矩阵估计方法

###### 主成分分析法

**样本**相关系数矩阵
$$
(\boldsymbol{R})_{D\times D}=\frac{\boldsymbol{X}\boldsymbol{X}^T}{N-1} \\
$$
$\lambda_k,u_k$ 分别为特征值与特征向量。其中 $u_k$ 需经过标准正交化

> 但是样本标准化之后的相关矩阵的特征向量应该说是单位正交的，编程时记得检查验证一下。

载荷矩阵估计为：
$$
\Lambda=[\sqrt{\lambda_1}u_1,\sqrt{\lambda_2}u_2,\cdots,\sqrt{\lambda_d}u_d ]
$$
特殊因子的方差估计
$$
\sigma_i^2=1-\sum_{j=1}^d\alpha_{ij}^2
$$

###### 主因子法

$R^*=R-D(\varepsilon)$，实际情况通常不知道特殊方差（上一中方法尽然用未知估计未知。。），可以采取如下估计：$\widehat{h}_i^2=\max |r_{ij}|,i \ne j$

再对 $R^*$ 分解估计载荷矩阵

###### 最大似然估计

matlab默认估计载荷矩阵方法，函数为 `factoran`

##### 因子旋转

使载荷矩阵结构简化

1. 方差最大法；
2. 四次方最大旋转
3. 等量最大法

#####  因子得分——估计公共因子

因子得分函数：
$$
F_j=c_j+\beta_{j1}X_1+\cdots+\beta_{jD}X_D,j=1,2,\cdots,d
$$
问题等价于：
$$
\min \quad(X-\mu-\Lambda F)^TD^{-1}(X-\mu-\Lambda F) \\
s.t. \quad X-\mu=\Lambda F+\varepsilon
$$
求得最优估计 $F=(\Lambda^TD^{-1}\Lambda)^{-1}\Lambda^TD^{-1}(X-\mu)$

> 此处未用粗体。。。。太多了



#### 判别分析 DA

LDA最开始是作为解决**二分类**问题由Fisher在1936年提出，由于**计算过程实际上对数据做了降维处理**，因此也可用作监督线性降维。它通过将高维空间数据投影到低维空间，在**低维**空间中确定每个样本所属的**类**，这里考虑 $K$ 个类的情况。它的目标是将样本能尽可能正确的分成 $K$ 类，体现为**同类样本投影点尽可能近，不同类样本点尽可能远**，这点跟PCA就不一样、PCA是希望所有样本在某一个维数上尽可能分开，LDA的低维投影**可能会重叠**，但是PCA就不希望投影点重叠。它采用的降维思路跟PCA是一样的，都是通过矩阵乘法来进行线性降维，投影点是
$$
Z_i=\boldsymbol{W}^T\cdot X_i
$$
![1](http://7xkmdr.com1.z0.glb.clouddn.com/lda.jpg)

由于希望属于不同类的样本尽可能离的远，那么就希望投影过后的**投影中心点**离的尽可能远，容易想到的目标是
$$
W=\arg \max ||W^T\mu_1-W^T\mu_2||^2
$$
但是这还不够，上图中直接投下来距离就足够大，但是出现了重叠。



设已有 $q$  个总体 $X_1,X_2,\cdots,X_q$ ，他们的分布函数为 $F_i(x),i=1,2,\cdots,q$ 。每个都是 $p$ 维函数。



##### 距离判别

距离判别是简单、直观的一种判别方法，该方法适用于**连续性随机变量**的判别类

###### 马氏距离

通常我们定义的距离是Euclid 距离（简称欧氏距离）。但在统计分析与计算中，Euclid 距离就不适用了，看一下下面的例子

![](https://i.loli.net/2018/07/30/5b5ece8f02424.jpg)

交点从欧几里得距离看更靠近0，但是考虑方差，他更靠近 $\mu_2$ ，即欧几里得未考率样本的**原始分布**信息，而马氏距离考虑了分布，它由协方差定义
$$
d(\boldsymbol{x},\boldsymbol{y})=\sqrt{(\boldsymbol{x}-\boldsymbol{y})^T\boldsymbol{\Sigma^{-1}}(\boldsymbol{x}-\boldsymbol{y})}
$$
同理可以定义样本与总体 $A​$ 的距离：
$$
d(\boldsymbol{x},\boldsymbol{A})=\sqrt{(\boldsymbol{x}-\boldsymbol{y})^T\boldsymbol{\Sigma^{-1}}(\boldsymbol{x}-\boldsymbol{y})}
$$

###### 判别准则

设总体 $A,B$ 的均值向量分别为 $\boldsymbol{\mu_1},\boldsymbol{\mu_2}$ ，协方差矩阵分别为 $\boldsymbol{\Sigma_1} ,\boldsymbol{\Sigma_2}$

判断准则：
$$
\boldsymbol{x} \in\left\{ \begin{array}{l}
{\rm{A,}}\;w(\boldsymbol{x} ) \ge {\rm{0}}\\
B,\;w(\boldsymbol{x} ) < 0
\end{array} \right.
$$

###### 判断函数

需要分情况讨论

一、$\boldsymbol{\Sigma_1} =\boldsymbol{\Sigma_2}$
$$
w(x)=(x-\bar{\mu})\Sigma^{-1}(\mu_1-\mu_2)
$$
其中 $\bar{\mu}=(\mu_1+\mu_2)/2$

但是通常情况下总体分布是不知道的，相应统计量得用**样本数据来估计** 。

假设 $x_1^{(1)},x_2^{(1)},\cdots,x_{n_1}^{(1)}$ 为来自总体 $A$ 的 $n_1$ 个样本点，$x_1^{(2)},x_2^{(2)},\cdots,x_{n_2}^{(2)}$ 为来自总体 $B$ 的 $n_2$ 个样本点，
$$
\begin{align}
\mu_i&=\frac{1}{n_i}\sum_{j=1}^{n_i}x_j^{(i)} ,i=1,2   \\
\Sigma&=\frac{1}{n_1+n_2-2}(S_1+S_2)  \\
S_i&=\sum_{j=1}^{n_i}(x_j^{(i)}-\bar{x}^{(i)})(x_j^{(i)}-\bar{x}^{(i)})^T,i=1,2
\end{align}
$$
二、$\mu_1 \ne \mu_2,\mathbf{\Sigma_1} \ne\boldsymbol{\Sigma_2}$
$$
w(x)=(x-\mu_2)^T \Sigma_2^{-1}(x-\mu_2)-(x-\mu_1)^T \Sigma_1^{-1}(x-\mu_1)
$$
样本估计如下：
$$
\begin{align}
\mu_i&=\frac{1}{n_i}\sum_{j=1}^{n_i}x_j^{(i)} ,i=1,2   \\
\Sigma_i&=\frac{1}{n_i-1}S_i ,i=1,2 \\
S_i&=\sum_{j=1}^{n_i}(x_j^{(i)}-\bar{x}^{(i)})(x_j^{(i)}-\bar{x}^{(i)})^T,i=1,2
\end{align}
$$

##### Fisher 判别

Fisher 判别的基本思想是投影，即将表面上不易分类的数据通过投影到某个方向上，使得投影类与类之间得以分离的一种判别方法。

设两个 $p$ 维总体的 $X_1,X_2$ （均值向量分别为 $\mu_1,\mu_2$）,多元观测 $x$的线性组合 $y=a^Tx$ 。易得
$$
\mu_{y1}=a^T\mu_1 \\
\mu_{y2}=a^T\mu_2 \\
\sigma_y^2=a^T\Sigma a
$$
Fisher 的思想便是
$$
\max \quad\frac{(\mu_{y1}-\mu_{y2})^2}{\sigma_y^2}=\frac{[a^T(\mu_1-\mu_2)]^2}{a^T\Sigma a}
$$

> 在方差比较小的前提下拉大样本中心的距离

###### 判断准则

$$
\boldsymbol{x} \in\left\{ \begin{array}{l}
{\rm{X_1,}}\;w(x) \ge {\rm{0}}\\
X_2,\;w(x) < 0
\end{array} \right.
$$

###### 判断函数

$$
w(x)=[x-(\mu_1+\mu_2)/2]^T\Sigma^{-1}(\mu_1-\mu_2)
$$

##### 贝叶斯判别 Bayes

![](https://i.loli.net/2018/07/30/5b5f2e4e9f74f.png)

具体模型先略过

##### 判别分析的步骤

与多元回归类似，变量选择的好坏直接影响判别分析的效果。 

- 第1步：确定研究的问题与目的

  >  判别分析适合将**个体归类**的问题，特别适合一个定性的被解释变量和多个定量的解释变量的情形。

- 第2步：判别分析研究设计

  * 解释变量与被解释变量的选择：被**解释变量**的组数可以是两个或更多，但必须互斥和完备。
  * 样本容量：判别分析对样本量与预测变量的比率敏感。
  * 总样本量：建议比率为**每个解释变量20个观测**，最小的总样本量为每个变量5个观测。最小的组的大小必须超过解释变量的个数，建议每组至少有20个观测，还要注意组的相对大小（大的组有不相称的高的分类机会）。
  * 样本分割：需要将样本分割为两个子样本，**一个用于估计判别函数**，另一个用于**验证**。随机分组，最常见的是随机分为两半。通常各组比率相同。

- 第3步：判别分析的假定

  > 多元正态性，如不满足建议使用**Logistic**回归。Box’s Test 检验各组协方差阵是否相等，不等的协方差矩阵可能会负面影响分类过程，观测会被“过度归类”到大的协方差阵组中。解释变量的多重共线性。

- 第4步：估计判别模型和评估整体拟和**统计显著性**： 

  > * Wilks’ Lambda， Hotelling迹和Pillai评估判别函数的判别效力的显著性。
  > * 评估整体拟和：计算每个观测的判别Z得分，检验各组在判别Z得分上的差异，评估组，关系的预测精度。

- 第5步：结果的解释

  > 解释判别分析中每个解释变量的相对重要性。标准化判别权重（判别系数）：如存在多重共线性时不合适，可能不稳定。判别载荷，又称结构相关系数，是每个解释变量与判别函数的简单相关系数，也可能不稳定。偏F值。能力指数：当保留多个判别函数时。

- 第6步：结果的验证,分隔样本或交叉验证法。

#### 多维缩放 MDS（multi dimensional scaling ）

MDS应用在数据降维时，基本思想和上面的例子相同：**保证所有数据点对在低维空间中的距离等于在高维空间中的距离**。 最多用于将至2维用于可视化

设设计阵为 $X_{D \times N}$ ,距离**平方阵** $Q \in R^{N \times N}$ ，目标降维后设计阵为 $\bar{X}_{d \times N}$ 而距离平方阵尽可能不变

设 $B_{N \times N}$为降维之后的内积矩阵，即 $B_{ij}=<X_{di},X_{dj}>$  。[推倒过程](https://www.jianshu.com/p/3ef58cafbe9a)

![img](https://upload-images.jianshu.io/upload_images/4155986-5b97cedd25fa6652.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700)

> 于图中不符的是本处采用的输入为相互距离的平方

结论是
$$
B_{ij}=-0.5(Q_{ij}-\frac{1}{N}\sum_{k=1}^NQ_{ik}-\frac{1}{N}\sum_{k=1}^NQ_{kj}+\frac{1}{N^2}\sum_{k=1}^N\sum_{m=1}^NQ_{km})
$$


此时只要分解 $\boldsymbol{B}=\boldsymbol{U}^T\boldsymbol{U}$ 可得 $X_{d \times N}$ 。对其做特征值分解（一定是方阵）$\boldsymbol{B}=\boldsymbol{V}\boldsymbol{\Lambda}\boldsymbol{V}^T$ ，在其 $d$ 个特征值中，假定有 $d^*$ 个非零特征值，构成对角阵 $\boldsymbol{\Lambda}_*$ 

计 $\boldsymbol{V_* }$ 表示相应的特征向量矩阵。则：
$$
X_{d^* \times N}=\boldsymbol{\Lambda}_*^{1/2}\boldsymbol{V_*^T \in \mathbb{R}^{d^* \times N}}
$$
而这时完全保留了高维空间的相互距离关系，事实上大多数情况近似就够了，故可以取 $d \ll d^*$ ，比如取2、3来可视化。

![img](https://upload-images.jianshu.io/upload_images/4155986-6ed74a207dca3d3a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700)

#### t-SNE

参考文献：

```bibtex
@article{maaten2008visualizing,
  title={Visualizing data using t-SNE},
  author={Maaten, Laurens van der and Hinton, Geoffrey},
  journal={Journal of machine learning research},
  volume={9},
  number={Nov},
  pages={2579--2605},
  year={2008}
}
```

t-SNE 也是一种将高维数据降维到**二维或者三维**空间的非线性方法。非常适用于高维数据降维到2维或者3维，进行可视化。 方法的改进。t-SNE在低维空间下使用更重长尾分布的t分布来避免 crowding 问题和优化问题 

![show png](http://www.datakit.cn/images/statistics/norm_t_dict.png)



##### 算法流程

1. 输入：数据 $\boldsymbol{X}=[x_1,x_2,\cdots,x_n]$ ，计算损失函数的参数：困惑度 perp

2. 优化参数：迭代次数 $T$ ，学习速率 $\eta$ ，动量 $\alpha(t)$ 

3. 输出：降维后数据：$\boldsymbol{Y}^T=[y_1,y_2,\cdots,t_n]$ 

4. 开始优化：

   * 计算给定 perp 情况下的条件概率：
     $$
     p_{j|i}=\frac{\exp(-||x_i-x_j||^2/(2\sigma_i^2))}{\sum \limits_{k\ne i}\exp(-||x_i-x_k||^2/(2\sigma_i^2)))}
     $$

   * 令 $p_{ij}=\frac{p_{j|i}+p_{i|j}}{2n}$

   * 用 $N(0,10^{-4}I) $ 初始化 $\boldsymbol{Y}$ 

   * 迭代：从 1 到 $T$ ,进行如下操作

     * 计算低维度下的 $q_{ij}$ 
       $$
       q_{ij}=\frac{(1+||y_i-y_j||^2)^{-1}}{\sum\limits_{k \ne l}(1+||y_k-y_l||^2)^{-1}}
       $$
       

     * 计算梯度 $ \frac{\partial C }{\partial y_i}=4\sum_j(p_{ij}-q_{ij})(y_i-y_j)(1+||y_i-y_j||^2)^{-1}$

     * 更新 $\boldsymbol{Y}^t=\boldsymbol{Y}^{t-1}+\eta \frac{dC}{d \boldsymbol{Y}}+\alpha(t)(\boldsymbol{Y}^{t-1}-\boldsymbol{Y}^{t-2})$

   * 结束

5. 结束



##### 局限性

- 主要用于**可视化**，很难用于其他目的。比如测试集合降维，因为他没有显式的预估部分，**不能在测试集合直接降维**；比如降维到10维，因为t分布偏重长尾，1个自由度的t分布很难保存好局部特征，可能需要设置成更高的自由度。

- t-SNE倾向于保存局部特征，对于**本征维数(intrinsic dimensionality)**本身就很高的数据集，是不可能完整的映射到2-3维的空间

- t-SNE没**有唯一最优解**，且没有预估部分。**如果想要做预估，可以考虑降维之后，再构建一个回归方程之类的模型去做**。但是要注意，t-sne中距离本身是没有意义，都是概率分布问题。

  > 每次优化的结果都是不一样的，不能一次确定固定的模型，除非降维后建立一个回归、聚类、分类模型

- 训练太慢。有很多基于树的算法在t-sne上做一些改进



##### 代码参见 /code/tsne.py

![](https://i.loli.net/2018/08/01/5b61705789e82.png)





#### 降维效果的度量指标







### 模型的求解——编程





### 模型的建立——论文

