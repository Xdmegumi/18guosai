## 人工神经网络

***

### 模型原理——建模部分

#### 神经网络简介

人工神经网络是在现代神经科学的基础上提出和发展起来的，旨在反映人脑结构及功能的一种抽象数学模型。它在模式识别，**图像处理**，智能控制，**组合优化**，金融预测与管理，通信，机器人以及专家系统等领域得到广泛的应用，提出了40 多种神经网络模型，其中比较著名的有感知机，Hopfield 网络，Boltzman 机，自适应共振理论及反向传播网络（BP）等。

1. 人工神经元模型

   下图表示出了作为人工神经网络（artificial neural network，以下简称NN）的基本单元的神经元模型，它有三个基本要素：

   * 一组连接（对应于生物神经元的突触），连接强度由各连接上的权值表示，权值为正表示激活，为负表示抑制
   * 一个求和单元，用于求取各输入信号的加权和（线性组合）
   * 一个非线性激活函数，起非线性作用并将神经元输出幅度限制在一定范围内（一般限制在 (0,1) 或 (−1,1) 之间）。此外还有一个阈值 $\theta_k$

   以上作用可以用数学表达式表示出来：

$$
u_k=\sum_{j=1}^pw_{kj}x_j,\quad v_k=u_k-\theta_k,\quad y_k=\varphi(v_k)
$$

   激活函数 $\varphi (\cdot)​$ 可以有以下几种:

   * 阈值函数即阶梯函数

   * 分段线性函数

     > 它类似于一个放大系数为 1 的**非线性放大器**，当工作于线性区时它是一个线性组合器，放大系数趋于无穷大时变成一个阈值单元

   * **sigmoid 函数**

   MATLAB工具箱中的激活函数

|   函数名   |        功 能         |
| :--------: | :------------------: |
| `purelin`  |     线性传递函数     |
| `hardlim`  |    硬限幅传递函数    |
| `hardlims` |  对称硬限幅传递函数  |
|  `satlin`  |   饱和线性传递函数   |
| `satlins`  | 对称饱和线性传递函数 |
|  `logsig`  |   对数S 形传递函数   |
|  `tansig`  |   正切S 形传递函数   |
|  `radbas`  |  **径向基传递函数**  |
|  `compet`  |    竞争层传递函数    |

2. 网络结构及工作方式

   从作用效果看，前馈网络主要是函数映射，可用于**模式识别**和**函数逼近**。反馈网络按对能量函数的极小点的利用来分类有两种：第一类是能量函数的所有极小点都起作用，这一类主要用作各种联想存储器；第二类只利用全局极小点，它主要用于求解最优化问题




#### BP 神经网络

由于用得太多，使用时只需放一个结合问题背景的**拓扑结构图与算法步骤**

且一般不用做主模型。关键在于学会**调参**

![](https://i.loli.net/2018/08/02/5b62b3e8ef4e8.png)



为了更有效地应用 BP 算法，我们做出如下一些补充说明:



* 在如上的讨论中使用的是**最速下降法**，显然，这也不是唯一的选择，其它的非线性优化方法，诸如**共轭梯度法，拟牛顿法**等，都可用于计算。为了加速算法的收敛速度，还可以考虑各种不同的**修正**方式。
* BP 算法的出现，虽然对人工神经网络的发展起了重大推动作用，但是这一算法仍有很多问题．对于一个大的网络系统，BP 算法的工作量仍然是十分可观的，这主要在于算法的**收敛速度很慢**。更为严重的是，此处所讨论的是非线性函数的优化，那么它就无法逃脱该类问题的共同困难：BP 算法所求得的解，只能保证是依赖于初值选取的局部极小点。为克服这一缺陷，可以考虑改进方法，例如**模拟退火**算法，或从多个随机选定的初值点出发，进行多次计算，但这些方法都不可避免地加大了工作量。



##### 蠓虫分类问题的求解



```matlab
clear
p1=[1.24,1.27;1.36,1.74;1.38,1.64;1.38,1.82;1.38,1.90;
1.40,1.70;1.48,1.82;1.54,1.82;1.56,2.08];
p2=[1.14,1.82;1.18,1.96;1.20,1.86;1.26,2.00
1.28,2.00;1.30,1.96];
p=[p1;p2]';
pr=minmax(p);
goal=[ones(1,9),zeros(1,6);zeros(1,9),ones(1,6)];
plot(p1(:,1),p1(:,2),'h',p2(:,1),p2(:,2),'o')
net=newff(pr,[3,2],{'logsig','logsig'});
net.trainParam.show = 10;
net.trainParam.lr = 0.05;
net.trainParam.goal = 1e-10;
net.trainParam.epochs = 50000;
net = train(net,p,goal);
x=[1.24 1.80;1.28 1.84;1.40 2.04]';
y0=sim(net,p)
y=sim(net,x)
```



#### RBF 神经网络

径向基函数神经网络的优点：逼近能力，分类能力和学习速度等方面都优于 BP 神经网络，结构简单、训练简洁、学习收敛速度快、能够逼近任意非线性函数，克服局部极小值问题。

> 原因在于其参数初始化具有一定的方法，并非随机初始化。 

RBF 是具有单隐层的三层前向网络 ,隐藏层节点数视所描述问题的**需要**而定 ，隐藏层中神经元的变换函数即**径向基函数**是对中心点径向对称且衰减的非负线性函数， 该函数是**局部响应**函数 。输出层是对线性权进行调整，采用的是线性优化策略，因而学习速度较快；而隐含层是对激活函数（格林函数，高斯函数，一般取后者）的参数进行调整，采用的是非线性优化策略，因而学习速度较慢 。

> 论文此处需要画神经网络拓扑结构图~

输入层为 $\boldsymbol{X}=[x_1,x_2,\cdots,x_n]$ ，实际输出为 $\boldsymbol{Y}=[y_1,y_2,\cdots,y_p]$

 输出层第 $k$ 个神经元输出为
$$
y_k==\sum_{i=1}^mw_{ik}R_i(\boldsymbol{X}),k=1,2,\cdots,p
$$
其中隐含层的第 $i$ 个神经元的作用函数为
$$
R_i(\boldsymbol{X}）=\exp(-||\boldsymbol{X}-\boldsymbol{C}_i||^2/2\sigma_i^2),i=1,2,\cdots,m
$$
其中 $\boldsymbol{C_i}$ 为第 $i$ 个基函数的中心。



##### 训练步骤：

1. 确定基函数的中心 $C_i$ 利用一组输入来计算 $m$ 个$C_i,i=1,2,\cdots,m$ ，是数据点尽可能均有的抽样并且在 $C_i$ 处也密集。一般采用 kmeans 聚类
2. 确定基函数的宽度 $\sigma_i$ 。
3. 确定隐含层到到输出层的连接权重 $w_{ik}$ 。其修正可采用最小均方差误差准则进行



> 编程对比实现 BP 和 RBF ，并可视化模型结果与精确度，小黄书 428页。

#### VAE（变分自解码器）

参考：

```bibtex
@article{kingma2013auto,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}
```

原理似乎挺复杂的、。。

https://blog.csdn.net/JackyTintin/article/details/53641885

除了作为一个生成模型，他也有可以用为无监督学习降维，你先编程实现一下吧，可运行代码在 code/vae 下，（比较吃cpu),你看看能不能用在成电的那个题目上降维可视化一下~



## 编程

## 论文