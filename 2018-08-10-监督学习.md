## 监督学习

> 主要整理来自于：
>
> 1. [python机器学习](http://sklearn.apachecn.org/cn/0.19.0/supervised_learning.html#supervised-learning)
> 2. 《统计学习方法》

### 原理部分——建模

统计学习包括监督学习、非监

督学习、半监督学习以及强化学习。无监督学习主要包括前面总结的**降维与聚类**等不需要标签的算法。而监督学习的任务是**学习**一个模型，使给定的输入，对其相应的输出做出一个好的**预测**。

1. 输入与输出变量均为连续性变量则称之为**回归问题** 
2. 输出变量为有限个离散变量称之为**分类问题**
3. 输入、输出均为**变量序列**称之为**标注问题**（如隐马尔科夫中的预测问题）

设输入为随机变量 $X$ ，输出为随机变量 $Y$ ，统计学习假设数据具有一定规律，$X,Y$ 具有**联合概率分布**的假设是监督学习关于**数据的基本假设**

统计学习三要素：
$$
方法=模型+策略+算法
$$
下面简述监督学习中的三要素

#### 监督学习三要素

##### 模型

模型的假设空间（hypothesis space）包含所有可能的条件概率分布或者决策函数。假设假设空间用 $\mathcal{F} $ 表示，可以定义为决策函数的集合 $\mathcal{F} =\{f\mid Y=f(X)\}$ 。但是建模时一般只选取一种模型。

##### 策略

统计学习的目标在于从假设空间中选取最优模型。首先引入两个概念：损失函数度量模型一次预测的好坏，**分险函数**度量**平均意义下**的模型预测好坏。

损失函数是 $f(X)$ 和 $Y$ 的非负实值函数，记作 $L(f(X),Y)$ ，常见的损失函数如下：

1. 0-1损失函数，当且仅当 $Y=f(X)\rightarrow L(Y,f(X))=1$

2. 平方损失函数，即 $L=(f(X)-Y)^2$

3. 绝对损失函数，即 $L=|f(X)-Y|$

4. 对数损失函数或者对数似然损失函数
   $$
   L(Y,P(Y\mid X))=-\log P(Y\mid X)
   $$





损失函数的期望即分险函数或者期望损失
$$
R_{\exp}(f)=E_P[L(Y,f(X))]=\int \limits_{x \times y}L(y,f(x))P(x,y)dxdy
$$
学习的目标是选择期望损失最少的函数，然而 $P(X,Y) $ 是未知的。

给定一个训练集，模型 $f(X)$ 关于训练集的平均损失成为**经验风险**（empirical risk）或经验损失，记作 
$$
R_{emp}(f)=\frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))
$$
但是现实中训练样本有限，用经验风险估计期望损失并不理想，故而需要进行**矫正** ，这就是监督学习的两个基本策略：

1. 经验风险最小化
   $$
   \min \limits_{f \in \mathcal{F} }\quad\frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))
   $$

2. 结构风险最小化

   > 当样本足够大时，经验风险最小化能保证很好的学习效果。比如极大似然估计就是经验风险最小化的例子。当模型是条件概率分布，损失函数是对数似然函数时，两者等价。

   但是，当样本比较小，经验风险最小化学习的效果未必好，会产生“过拟合”。结构经验最下化等价于正则化，即在经验风险后加上表示**模型复杂度**的**正则化项或者罚项** 。
   $$
   \min \limits_{f \in \mathcal{F} }\quad\frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))+\lambda J(f)
   $$
   $J(f) $ 为模型的复杂度，是定义在假设空间上的泛函。模型越复杂其越大。当模型是条件概率分布，损失函数是对数损失函数，模型复杂度用模型的先验概率表示时，结构风险最小化等价于**最大后验概率估计**



##### 算法

#### 正则化与交叉验证

正则化可以避免模型复杂度过高出现的过拟合，因此通常正则化项是模型参数的范数，这样只要有参数可以取零（复杂度降低），损失函数将会显著下降

例如，回归问题中，损失函数是平方损失，正则化项可以是参数向量的 $L_2$ 范数
$$
\min \limits_{w}\quad L(w)=\frac{1}{N}\sum_{i=1}^NL(y_i-f(x_i;w))+\frac{\lambda}{2}||w||^2
$$
从贝叶斯的角度来看，正则化项对应着模型的先验概率。可以假设1复杂模型具有较大的先验概率。

##### 交叉验证

另一中模型选择的方法便是交叉验证，这里指的是选择不同的模型，通常情况下是不同的参数导致不同的模型，一种手段是训练集不同的划分可能会得到不同的参数，第二种是模型的设置不同（定义不同个数参数的模型），进行训练从而比较不同的模型，后者称为简单交叉验证

> 简单交叉验证这真的是。。交叉了？不就是变更模型了吗。。

![](https://i.loli.net/2018/08/10/5b6d6545d2033.png)



#### 生成模型与判别模型

* 生成模型：由数据学习联合概率分布 $P(X,Y)$ ，然后求出条件概率分布作为预测的模型
  $$
  P(Y\mid X)=\frac{P(X,Y)}{P(X)}
  $$

  > 可以还原联合概率分布；学习收敛速度更快；样本增加时更快逼近真实模型；当存在隐变量时任然适用；

* 判别模型：直接学习决策函数或者条件概率分布作为预测模型

  > 往往学习准确率更高；简化学习问题

这里先放一张总结图：

![](https://i.loli.net/2018/08/10/5b6d6ac0d0b4d.png)



#### 广义线性模型（GLM）

**指数族分布**，我们定义一类分布
$$
p(y;\eta)=b(y)e^{\eta^TT(y)-a(\eta)}
$$
其中 $\eta$ 称为该分布的自然参数（natural parameter）或者标准参数，通常是一个实数或者一个矩阵；$T(y)$ 称为充分统计量，仅仅依赖于样本 $y_1,y_2,\cdots,y_n$ ，它不含总体分布的任何未知参数，通常情况下 $T(y)=y$ ；$a(\eta) $ 为累计函数，主要负责归一化，使得 $0<p(y;\eta)<1$ 。

从而指数族分布就是三个函数 $a,b,T$ ，一个参数 $\eta$ 。绝大多数分布在这个族内

以伯努利分布为例，设 $y \sim B(\varphi )$
$$
\begin{align}
P(y;\eta)&=\varphi^y(1-\varphi)^{1-y} \\
&=e^{y\log(\varphi)+(1-y)\log(1-\varphi)} \\
&=e^{y\log(\frac{\varphi}{1-\varphi})+\log(1-\varphi)} \end{align}
$$
从而 $T(y)=y;\eta=\log(\frac{\varphi}{1-\varphi});b(y)=1;a(y)=-\log(1-\varphi)$ 。逆向求解 
$$
\varphi=\frac{1}{1+e^{-\eta}}
$$
这正是 **Sigmoid** 函数。上述验证了伯努利分布是指数族分布。

> logistic 二分类中的 $y$ 显然服从伯努利分布

不管是回归问题还是分类问题，我们可以推广到广义线性回归模型需要以下三个假设

1. $y \mid x;\theta \sim \mbox{ExponentialFamily}(\eta)  $ 。即假设需要预测的 $y$ 在给定的 $x,\theta$ ，属于以 $\eta$ 为参数的指分布族
2. 对与给定的 $x$ ，我们的目标是预测 $T(y)$ 的预测值 。在很多例子中 $T(y)=y$ 。这便意味着预测结果 $h(x)$ 满足 $h(x)=E[y \mid x]$ 
3. 自然参数 $\eta$ 与 $x$ 为线性关系，即 $\eta=\theta^Tx$

##### 回归

###### 普通最小二乘

根据 GLM 第一个假设 ，对于给定的 $x$ ，$y \sim N(\mu,\sigma^2)$ ，根据假设二、三，我们知道 
$$
\widehat{y}=h_{\theta}(x)=E[y \mid x;\theta]=\mu=\eta=\theta^Tx
$$

> 正态分布作为指数族分布参数 $\eta$ 即期望为一结论易证

也就是说用最小二乘法估计**线性回归**的参数是需要对于给定的 $x$ 假设 $y$ 服从正态分布

> 事实上，最小二乘法的合理性可以通过极大似然法给出解释：（假设自变量为1维）
>
> 设 $y=\theta^Tx+\varepsilon$ ，而 $\mu=\theta^T x$ ，故而可假设 $\varepsilon \sim N(0,\sigma^2)$ .
>
> 从而似然函数 
> $$
> L(\theta)=\prod_{i=1}^np(y(i)\mid x(i);\theta) \\
> =\prod_{i=1}^n\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(y(i)-\theta^T x(i))^2}{2\sigma^2})
> $$
> 最大化上述似然函数的对数，化简后**等价于**最小化最小二乘法

然而，对于普通最小二乘的系数估计问题，其依赖于模型各项的相互**独立性**。当各项是相关的，且设计矩阵（样本矩阵） $X$ 的各列近似线性相关，那么，设计矩阵会趋向于奇异矩阵，这会导致最小二乘估计对于随机误差非常敏感，产生很大的**方差**。

例如，在没有实验设计的情况下收集到的数据，这种**多重共线性**（multicollinearity）的情况可能真的会出现。 可有一些方法。

###### 岭（Ridge）回归

参考：[python机器学习](http://sklearn.apachecn.org/cn/0.19.0/modules/linear_model.html#ordinary-least-squares)

[`Ridge`](http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge) 回归通过对系数的大小施加惩罚来解决 [普通最小二乘法] 的过拟合和多重共线性问题。 岭系数最小化的是带**罚项**的残差平方和

其模型为
$$
\min \limits_{\boldsymbol{\theta}}\quad L(\boldsymbol{\theta})=\frac{1}{N}\sum_{i=1}^N(y_i-\boldsymbol{\theta}^T\boldsymbol{x}_i)^2+\alpha||\boldsymbol{\theta}||_2^2
$$

> 其中每一个 $x_i$ 均为与 $\theta$ 等长的**向量**

其中 $\alpha \ge 0$ 是控制系数收缩量的复杂性参数：$\alpha$ 的值越大，系数向量越趋近于零，模型越简单，系数对**多重共线性的鲁棒性**越强。

![../_images/sphx_glr_plot_ridge_path_0011.png](http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_ridge_path_0011.png)

 实际中正则化参数 $\alpha$ 通过交叉验证求解，python中可以用 `RidgeCV `求解

```python
>>> from sklearn import linear_model
>>> reg = linear_model.RidgeCV(alphas=[0.1, 1.0, 10.0])
>>> reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])       
RidgeCV(alphas=[0.1, 1.0, 10.0], cv=None, fit_intercept=True, scoring=None,
    normalize=False)
>>> reg.alpha_                                      
0.1
```

该对象与 GridSearchCV 的使用方法相同，只是它默认为 Generalized Cross-Validation(广义交叉验证 GCV)，这是一种有效的**留一验证方法**（LOO-CV）: 

> 看看有没有 $S$ 折交叉验证的参数项，留一交叉验证一般适用于1数据较少时

###### Lasso回归

The [`Lasso`](http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso) 是估计**稀疏系数**的线性模型。 它在一些情况下是有用的，因为它倾向于使用具有较少参数值的情况，有效地**减少**给定解决方案所依赖**变量**的数量。 因此，Lasso 及其变体是压缩感知领域的基础。 在**一定条件**下，它可以**恢复**一组**非零权重**的精确集 
$$
\min \limits_{\boldsymbol{\theta}}\quad L(\boldsymbol{\theta})=\frac{1}{2N}\sum_{i=1}^N(y_i-\boldsymbol{\theta}^T\boldsymbol{x}_i)^2+\alpha||\boldsymbol{\theta}||_1
$$

###### 多项式回归

实际上利用**基函数**可以将多项式回归化为线性回归

![\hat{y}(w, x) = w_0 + w_1 x_1 + w_2 x_2 + w_3 x_1 x_2 + w_4 x_1^2 + w_5 x_2^2](http://sklearn.apachecn.org/cn/0.19.0/_images/math/1e1f74179df321954b823943c08d555a524e69f9.png)

设 $z=[x_1,x_2,x_1x_2,x_1^2,x_2^2]$ ，则

![\hat{y}(w, x) = w_0 + w_1 z_1 + w_2 z_2 + w_3 z_3 + w_4 z_4 + w_5 z_5](http://sklearn.apachecn.org/cn/0.19.0/_images/math/618623438e33ecf053d364a0b938f056cd758b34.png)

##### 分类

###### logistic 回归

logistic 回归，虽然名字里有 “回归” 二字，但实际上是解决分类问题的一类线性模型。

作为一个广义线性模型，对于给定的 $x$ ，$y$ 服从二项分布 $B(\varphi)$ ，并且由上文知
$$
\widehat{y}=E[y\mid x;\theta]=\varphi=\frac{1}{1+e^{-\eta}}=\frac{1}{1+e^{-\theta^Tx}}
$$

> scikit-learn 中 logistic 回归在 [`LogisticRegression`](http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression) 类中实现了二分类（binary）、一对多分类（one-vs-rest）及多项式 logistic 回归，并带有可选的 L1 和 L2 正则化。 
>
> 注意实现后的可视化

###### softmax 回归

> 参考：《机器学习斯坦福公开课》

此处我们的分类结果为 $k$ 类，在二分类中，$\varphi$ 刻画了是否取1的概率，显然多元分类仅仅依靠一个 $\varphi$ 是不够的，我们选取 $k-1$ 个参数来表示取第 $i$ 类的概率，下面将伯努利分布推广至多项的情形：

> 原本应该用 $k$ 个参数，但是概率和为1，所以可以减小一个自由度

为了更好的表示多项指数族分布，定义 $T(y) \in R^{k-1}$ 如下：

![](https://i.loli.net/2018/08/10/5b6d9ed6b8a10.jpg)

由实行函数可以表达为 $\mathbf{1}\{y=i\}=T(y)_i$ ，因此 $E[T(y)_i]=P\{y=i\}=\varphi_i$ 。

由独立事件的乘法法则有
$$
\begin{align}
p(y;\varphi)&=\varphi_1^{\mathbf{1}\{y=1\}}\varphi_2^{\mathbf{1}\{y=2\}}\cdots\varphi_k^{\mathbf{1}\{y=k\} } \\
&=\varphi_1^{\mathbf{1}\{y=1\}}\varphi_2^{\mathbf{1}\{y=2\}}\cdots\varphi_k^{1-\sum \limits_{i=1}^{k-1}T(y) _i} \\
&=b(y)e^{\eta^TT(y)-a(\eta)}   \\
  \end{align}
$$
其中
$$
\begin{align}
\eta&  = \left[ {\begin{array}{*{20}{c}}
{\log ({\varphi _1}/{\varphi _k})}\\
{\log ({\varphi _2}/{\varphi _k})}\\
 \vdots \\
{\log ({\varphi _{k - 1}}/{\varphi _k})}
\end{array}} \right] \\
a(\eta) &=-\log(\varphi_k) \\
b(y)&=1 
\end{align}
$$
由 $\eta$ 表达式可得 $\varphi_i=\varphi_k e^{\eta_i}$ ,由 $\sum \varphi_i=1$ 可得
$$
\varphi_i=\frac{e^{\eta_i}}{\sum \limits_{i=1}^ke^{\eta_i}}
$$
改方程将 $\varphi_i$ 和 $\eta_i=\theta_i^Tx$ 联系了起来，称之为 softmax 函数。
$$
p(y=i \mid x;\theta)=\varphi_i=\frac{\exp(\theta_i^Tx)}{\sum \limits_{i=1}^k\exp(\theta_i^Tx)}
$$
称之为 softmax 回归。同样可以使用最大似然法估计参数。不过这里的参数有 $k$ 个**向量** ，即 $\theta_i,i=1,2,\cdots,k$ 。其中 $\boldsymbol{\theta}_k=\boldsymbol{0}$ 

> 如果类别间相互排斥，可以选择softmax 分类器，如果不互斥，即可以一对多分类，选择 $K$ 个独立的二分类的 logistic 分类器更加合适。编程时看看 scikit-learn 中的一对多分类用的啥方法~

 

#### 支持向量机（SVM）

> 程序通过libsvm 或者 python实现,不要自己去实现下面提到的一些算法

##### 分类

通常说的支持向量机是一种二分类模型，他的基本模型是定义在特征空间上的**间隔最大**的**线性**分类器；支持向量机还包括**核技巧**，这使他称为实质上的非线性分类器。支持向量机的学习算法是求解**凸二次规划**的**最优化算法** 。

支持向量机学习方法包含构建由简至繁的过程：

1. 当数据线性可分时，通过**硬间隔最大化**学习一个线性分类器
2. 当数据近似线性可分时，通过软间隔最大化也学习一个线性分类器，称为软间隔支持向量机
3. 当数据线性不可分时，通过使用核技巧即软间隔最大化学习非线性支持向量机

###### 线性可分与硬间隔最大化

一般的，当数据线性可分时，存在无数的分离超平面可将数据正确分开，感知机利用误分类最小的策略求得最优超平面，但是解仍有无穷多个。支持向量机利用间隔最大化求解的最优分离面是**唯一的**（凸二次规划）

一般来说，一个点距离分离平面的远近可以表示分类预测的**确信程度** 。

**函数间隔：** 对于给定的训练数据集 $T$ 与超平面 $(w,b)$ ，定义超平面 $(w,b)$ 关于样本点 $(x_i,y_i)$ 的函数间隔为
$$
\gamma_i=y_i(wx_i+b)
$$

> 注意此处 $y_i​$ 不是纵坐标，而是监督学习的标签，二分类中对应于 $1,-1​$ 。若 $wx_i+b=-2,y_i=1​$，则预测错误， $\gamma_i=-2​$  。正负号衡量是否分类正确，绝对值衡量可信程度

**几何间隔：** 对法向量 $w$ 做个在正规约束，$||w||=1$ ，则函数间隔变为几何间隔

此即点到直线的欧几里得距离
$$
\gamma_i=y_i(\frac{wx_i+b}{||w||})
$$
定义超平面 $(w,b) $ 关于训练数据集 $T$ 的几何间隔为左右样本点中几何间隔的最小值
$$
\gamma =\min\limits_{i=1,2,\cdots,n}\gamma_i
$$
最大间隔分离超平面（几何间隔最大化）
$$
\max \limits_{w,b} \quad \gamma \\
s.t.  \quad y_i(\frac{wx_i+b}{||w||})\ge \gamma
$$
又由函数间隔和几何间隔的关系可转化为（见《统计学习方法》）以下凸二次规划
$$
\min \limits_{w,b} \quad \frac{1}{2}||w||^2 \\
s.t. \quad y_i(w_ix+b)-1\ge0
$$
**支持向量：** 满足 $w_ix+b=1$ 的点，由定义以及约束规划极值的 KT 条件求得

线性可分支持向量机**原规划及其对偶规划**算法：

![](https://i.loli.net/2018/08/11/5b6e45431c5cb.png)



###### 近似线性可分与软间隔最大化

上述算法对近似线性可分的数据是不适用的，因为不能保证约束条件都能满足。通常情况下数据中有一些特异点，去除这些点后大部分是线性可分的。为了解决这个问题，可以对每一个样本点在原来约束的基础上引进一个松弛变量 $\xi_i \ge0$ 使得
$$
y_i(w_ix+b)\ge1-\xi_i
$$
从而线性不可分的线性支持向量机的学习问题变成以下凸二次规划问题（原始问题）
$$
\min \limits_{w,b,\xi} \quad \frac{1}{2}||w||^2 +C\sum_{i=1}^n\xi_i\\
\begin{align}
s.t. \quad y_i(w_ix+b)&\ge1-\xi_i,\quad i=1,2,\cdots,n \\
\xi_i &\ge0,\quad i=1,2,\cdots,n
\end{align}
$$
其中 $C$ 为惩罚系数，可以通过交叉验证求解。理论证明 $w$ 唯一，但是 $b$ 得解存在于一个区间。

![](https://i.loli.net/2018/08/11/5b6e4971f1036.png)

显然近似可分的具有更广的实用性，但是在此时的支持向量更加复杂，如何进行模型分类结果好坏的**评估**呢：仍然以分离超平面为界，**本类**间隔平面与超平面之间不算误分

![](https://i.loli.net/2018/08/11/5b6e4a8747e8c.png)

###### 非线性支持向量机与核方法

核函数的定义：

设 $\mathcal{X}$ 是输入空间（欧式空间的子集或者离散集合），又设 $\mathcal{H}$ 为特征空间（希尔伯特空间），如果存在一个从 $\mathcal{X}$ 到 $\mathcal{H}$ 的映射
$$
\phi(x):\mathcal{X} \rightarrow\mathcal{H}
$$
使得对所有的 $x,z\in \mathcal{X}$ ，函数  $K(x,z)$ 满足条件
$$
K(x,z)=\phi(x)\cdot\phi(z)
$$
则称 $K(x,z)$ 为核函数，$\phi(x) $ 为映射函数，上式中 $\cdot$ 为希尔伯特空间中的内积

核技巧的想法是在学习与预测中只定义核函数而不显示的定义映射函数

那么如何将核技巧应用到支持向量机中呢？

我们注意到在线性支持向量机的**对偶**问题中，无论是目标函数还是决策函数都只涉及输入实例与实例之间的内积 $x_i \cdot x_j$ ，如果把这个用核函数  $K(x_i,x_j)$ 代替，那么相当于对核函数对应的**特征空间进行了线性可分**，而当**核函数为非线性**的时候，它的**原像**便实现了非线性可分！这就是核技巧

常用核函数：

1. 多项式核函数（polynomial kernel function）
   $$
   K(x,z)=(x\cdot z+1)^p
   $$
   对应的分类器是一个 $p$ 次多项式分类器。

   > 编程时记得画出原像空间的支持向量图（以多项式曲线为分割面）

2. 高斯核函数
   $$
   K(x,z)=\exp(-\frac{||x-z||^2}{2\sigma^2})
   $$
   此时的支持向量机是高斯径向基函数（radial basis function:RBF）分类器

   > 此核函数为RBF神经网络的中间作用函数，但是一般不用神经网络，因为其解释性比较差

3. 字符串核函数

   核函数不仅可以定义在欧式空间上，还可以定义在离散数据的集合上。比如，字符串核是定义在字符串集合上的核函数。详细过程参考《统计学习方法》

   > 应用可参考 /数学原理/The spectrum kernel a string kernel for SVM protein classification.pdf

学习方法如下：

![](https://i.loli.net/2018/08/11/5b6e601779c7a.png)

最优化方法可以参考启发式算法： **SMO 算法**

![](https://i.loli.net/2018/08/11/5b6e610b1309b.png)

###### svm 多元分类

SVM本身是一个二值分类器，SVM算法最初是为二值分类问题设计的，当处理多类问题时，就需要构造合适的多类分类器。

1. 直接法，直接在目标函数上进行修改，将多个分类面的参数求解合并到一个最优化问题中，通过求解该最优化问题“一次性”实现多类分类。这种方法看似简单，但其计算**复杂度比较高**，实现起来比较困难，只适合用于小型问题中，**淘汰**

2. 间接法，主要是通过**组合多个二分类器**来实现多分类器的构造，常见的方法有one-against-one和one-against-all两种。

   * 一对多法（one-versus-rest,简称OVR SVMs）

     训练时依次把某个类别的样本归为一类,其他剩余的样本归为另一类，这样 $k$ 个类别的样本就构造出了 $k$ 个SVM 。

     > 但是这种方法存在偏差，因而**不是很实用**

   * 一对一法（one-versus-one,简称OVO SVMs或者pairwise） 

     **Libsvm** 中的多类分类就是根据这个方法实现的。 

> 因此主要可以参考周老师给的 **libsvm** 和 python 机器学习
>
> sklearn 中的 SVC 函数是基于 libsvm 实现的，所以在参数设置上有很多相似的地方。（PS: libsvm中的二次规划问题的解决算法是SMO）



##### 回归

Support Vector Regression (SVR) using linear and non-linear kernels

其损失函数如下：
$$
err(x_i,y_i)=\max \{0,|wx_i+b|-\varepsilon\}
$$
$\varepsilon$ 描述的零损失区域的宽度，定义这个区域内的点损失为 0，这个区域以外的点的损失是点到**区域边界**的距离，这些区域外的点（或者有可能边界上的点）就是svr 的support vector。 svr 就是要找一条线，**忽略**它周围的点，对剩余的点进行回归。 

SVR比较简洁， 并不像我们谈到的逻辑帝回归那样需要迭代，并且这里的最终结果总是可逆的，唯一的不足就是时间复杂度，数据样本的三次方啊，三次方啊。要是有一千个数据样本你怎么办？所以和罗斯蒂回归相比较，SVR在数据**样本相对比较大**的时候几乎是一个灾难，而罗斯蒂回归在**数据的维度**很大的时候是一个灾难。 

参考[python机器学习](http://sklearn.apachecn.org/cn/0.19.0/auto_examples/svm/plot_svm_regression.html#sphx-glr-auto-examples-svm-plot-svm-regression-py)



#### 朴素贝叶斯——多分类

朴素贝叶斯法是基于贝叶斯定理与**特征条件独立**假设的分类方法。

设输入特征向量为 $x \in R^n$ ，输出为类标记 $y \in \{c_1,c_2,\cdots,c_K\}$ 。$X,Y$ 分别为输入输出空间的随机变量。训练数据 $T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_n,y_n)\}$ 由 $P(X,Y) $ 独立同分布产生

朴素贝叶斯算法通过训练数据学习联合概率分布 $P(X,Y) $ ，故而是一种生成模型

条件概率分布
$$
P(X=x\mid Y=c_k)=P(X^{(1)}=x^{(1)},\cdots,X^{(n)}=x^{(n)}\mid Y=c_k),\quad k=1,2,\cdots,K
$$
这里具有指数级的参数，朴素贝叶斯对条件概率做了一个条件独立性假设，这个是一个很强的假设，并也是它叫“朴素”的原因。
$$
P(X=x\mid Y=c_k)=P(X^{(1)}=x^{(1)},\cdots,X^{(n)}=x^{(n)}\mid Y=c_k) \\
=\prod_{j=1}^nP(X^{(j)}=x^{(j)}\mid Y=c_k)
$$
这一假设使得朴素贝叶斯变得简单，但是有时会丧失一些转确率。其分类时，对于给定的输入 $x​$ ，通过学习到的模型计算后验概率分布 $P(Y=c_k\mid X=x)  ​$ ，将后验概率最大的类作为 $x​$ 的类输出，后验概率由贝叶斯定理计算
$$
P(Y=c_k\mid X=x)  =\frac{P(X=x\mid Y=c_k)P(Y=c_k)}{\sum \limits_{k}P(X=x\mid Y=c_k)}\\
=\frac{P(Y=c_k)\prod \limits_{j}P(X^{(j)}=x^{(j)}\mid Y=c_k)}{\sum \limits_{k}P(Y=c_k)\prod \limits_{j}P(X^{(j)}=x^{(j)}\mid Y=c_k)}
$$
分类器可以表示为
$$
y=f(x)=arg\max \limits_{c_k}P(Y=c_k\mid X=x)
$$

> 即将实例分到后验概率最大的类中，这和期望风险最小化是等价的，参加《统计学习方法》64页

##### 朴素贝叶斯的参数估计问题

学习意味着估计 $P(Y=c_k)$ 和 $P(X^{(j)}=x^{(j)}\mid Y=c_k)$ 可以应用极大似然估计。但是会导致部分概率为零，解决方法是采用贝叶斯估计，在随机变量的频数上附加一个正数 $\lambda$ ，当 $\lambda=0$ 便是极大思然估计。常取 $\lambda=1$ ，这时称为**拉普拉斯平滑**



**朴素贝叶斯算法流程**：$N$ 为样本数，$n$ 为特征数



输入：训练数据 $T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$ ,其中 $x_i=(x_i^{(1)},x_i^{(2)},\cdots,x_i^{(n)})$ ，$x_i^{(j)}$ 是第 $i$ 个样本的第 $j$ 个特征，$x_i^j \in\{a_{j1},\cdots,a_{jS_j}\}$ ，$a_{jl}$ 是第 $j$ 个特征的第 $l$ 个值，$l=1,2,\cdots,S_j$ ，$y_i \in \{c_1,\cdots,c_K\}$ ；$\lambda$ ；实例 $x$

输出：实例 $x$ 的分类

开始：

1. 计算先验概率和条件概率：
   $$
   P(Y=c_k)=\frac{\sum \limits_{i=1}^N\mathbf{1}(y_i=c_k)+\lambda}{N+K\lambda }\\
   P(X^{(j)}=a_{ji}\mid Y=c_k)=\frac{\sum \limits_{i=1}^N\mathbf{1}(x_i^{(j)}=a_{ji},y_i=c_k)+\lambda}{\sum \limits_{i=1}^N\mathbf{1}(y_i=c_k)+S_j \lambda}
   $$

2. 计算后验概率
   $$
   P(Y=c_k\mid X=x)  =P(Y=c_k)\prod \limits_{j}P(X^{(j)}=x^{(j)}\mid Y=c_k)
   $$

3. 确定分类
   $$
   y=f(x)=arg\max \limits_{c_k}P(Y=c_k\mid X=x)
   $$

   > 从以上推导可以看出朴素贝叶斯适合于离散特征的快速分类

   

#### $k$ 近邻

#### 决策树

#### 线性与二次判别分析

#### 集成方法





#### 标注算法