## 监督学习

> 主要整理来自于：
>
> 1. [python机器学习](http://sklearn.apachecn.org/cn/0.19.0/supervised_learning.html#supervised-learning)
> 2. 《统计学习方法》

### 原理部分——建模

统计学习包括监督学习、非监督学习、半监督学习以及强化学习。无监督学习主要包括前面总结的**降维与聚类**等不需要标签的算法。而监督学习的任务是**学习**一个模型，使给定的输入，对其相应的输出做出一个好的**预测**。

1. 输入与输出变量均为连续性变量则称之为**回归问题** 
2. 输出变量为有限个离散变量称之为**分类问题**
3. 输入、输出均为**变量序列**称之为**标注问题**（如隐马尔科夫中的预测问题）

设输入为随机变量 $X$ ，输出为随机变量 $Y$ ，统计学习假设数据具有一定规律，$X,Y$ 具有**联合概率分布**的假设是监督学习关于**数据的基本假设**

统计学习三要素：
$$
方法=模型+策略+算法
$$
下面简述监督学习中的三要素

#### 监督学习三要素

##### 模型

模型的假设空间（hypothesis space）包含所有可能的条件概率分布或者决策函数。假设假设空间用 $\mathcal{F} $ 表示，可以定义为决策函数的集合 $\mathcal{F} =\{f\mid Y=f(X)\}$ 。但是建模时一般只选取一种模型。

##### 策略

统计学习的目标在于从假设空间中选取最优模型。首先引入两个概念：损失函数度量模型一次预测的好坏，**分险函数**度量**平均意义下**的模型预测好坏。

损失函数是 $f(X)$ 和 $Y$ 的非负实值函数，记作 $L(f(X),Y)$ ，常见的损失函数如下：

1. 0-1损失函数，当且仅当 $Y=f(X)\rightarrow L(Y,f(X))=1$

2. 平方损失函数，即 $L=(f(X)-Y)^2$

3. 绝对损失函数，即 $L=|f(X)-Y|$

4. 对数损失函数或者对数似然损失函数
   $$
   L(Y,P(Y\mid X))=-\log P(Y\mid X)
   $$



损失函数的期望即分险函数或者期望损失
$$
R_{\exp}(f)=E_P[L(Y,f(X))]=\int \limits_{x \times y}L(y,f(x))P(x,y)dxdy
$$
学习的目标是选择期望损失最少的函数，然而 $P(X,Y) $ 是未知的。

给定一个训练集，模型 $f(X)$ 关于训练集的平均损失成为**经验风险**（empirical risk）或经验损失，记作 
$$
R_{emp}(f)=\frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))
$$
但是现实中训练样本有限，用经验风险估计期望损失并不理想，故而需要进行**矫正** ，这就是监督学习的两个基本策略：

1. 经验风险最小化
   $$
   \min \limits_{f \in \mathcal{F} }\quad\frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))
   $$

2. 结构风险最小化

   > 当样本足够大时，经验风险最小化能保证很好的学习效果。比如极大似然估计就是经验风险最小化的例子。当模型是条件概率分布，损失函数是对数似然函数时，两者等价。

   但是，当样本比较小，经验风险最小化学习的效果未必好，会产生“过拟合”。结构经验最下化等价于正则化，即在经验风险后加上表示**模型复杂度**的**正则化项或者罚项** 。
   $$
   \min \limits_{f \in \mathcal{F} }\quad\frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))+\lambda J(f)
   $$
   $J(f) $ 为模型的复杂度，是定义在假设空间上的泛函。模型越复杂其越大。当模型是条件概率分布，损失函数是对数损失函数，模型复杂度用模型的先验概率表示时，结构风险最小化等价于**最大后验概率估计**



##### 算法

#### 正则化与交叉验证

正则化可以避免模型复杂度过高出现的过拟合，因此通常正则化项是模型参数的范数，这样只要有参数可以取零（复杂度降低），损失函数将会显著下降

例如，回归问题中，损失函数是平方损失，正则化项可以是参数向量的 $L_2$ 范数
$$
\min \limits_{w}\quad L(w)=\frac{1}{N}\sum_{i=1}^NL(y_i-f(x_i;w))+\frac{\lambda}{2}||w||^2
$$
从贝叶斯的角度来看，正则化项对应着模型的先验概率。可以假设1复杂模型具有较大的先验概率。

##### 交叉验证

另一中模型选择的方法便是交叉验证，这里指的是选择不同的模型，通常情况下是不同的参数导致不同的模型，一种手段是训练集不同的划分可能会得到不同的参数，第二种是模型的设置不同（定义不同个数参数的模型），进行训练从而比较不同的模型，后者称为简单交叉验证

> 简单交叉验证这真的是。。交叉了？不就是变更模型了吗。。

![](https://i.loli.net/2018/08/10/5b6d6545d2033.png)



#### 生成模型与判别模型

* 生成模型：由数据学习联合概率分布 $P(X,Y)$ ，然后求出条件概率分布作为预测的模型
  $$
  P(Y\mid X)=\frac{P(X,Y)}{P(X)}
  $$

  > 可以还原联合概率分布；学习收敛速度更快；样本增加时更快逼近真实模型；当存在隐变量时任然适用；

* 判别模型：直接学习决策函数或者条件概率分布作为预测模型

  > 往往学习准确率更高；简化学习问题

这里先放一张总结图：

![](https://i.loli.net/2018/08/10/5b6d6ac0d0b4d.png)



#### 广义线性模型（GLM）

**指数族分布**，我们定义一类分布
$$
p(y;\eta)=b(y)e^{\eta^TT(y)-a(\eta)}
$$
其中 $\eta$ 称为该分布的自然参数（natural parameter）或者标准参数，通常是一个实数或者一个矩阵；$T(y)$ 称为充分统计量，仅仅依赖于样本 $y_1,y_2,\cdots,y_n$ ，它不含总体分布的任何未知参数，通常情况下 $T(y)=y$ ；$a(\eta) $ 为累计函数，主要负责归一化，使得 $0<p(y;\eta)<1$ 。

从而指数族分布就是三个函数 $a,b,T$ ，一个参数 $\eta$ 。绝大多数分布在这个族内

以伯努利分布为例，设 $y \sim B(\varphi )$
$$
\begin{align}
P(y;\eta)&=\varphi^y(1-\varphi)^{1-y} \\
&=e^{y\log(\varphi)+(1-y)\log(1-\varphi)} \\
&=e^{y\log(\frac{\varphi}{1-\varphi})+\log(1-\varphi)} \end{align}
$$
从而 $T(y)=y;\eta=\log(\frac{\varphi}{1-\varphi});b(y)=1;a(y)=-\log(1-\varphi)$ 。逆向求解 
$$
\varphi=\frac{1}{1+e^{-\eta}}
$$
这正是 **Sigmoid** 函数。上述验证了伯努利分布是指数族分布。

> logistic 二分类中的 $y$ 显然服从伯努利分布

不管是回归问题还是分类问题，我们可以推广到广义线性回归模型需要以下三个假设

1. $y \mid x;\theta \sim \mbox{ExponentialFamily}(\eta)  $ 。即假设需要预测的 $y$ 在给定的 $x,\theta$ ，属于以 $\eta$ 为参数的指分布族
2. 对与给定的 $x$ ，我们的目标是预测 $T(y)$ 的预测值 。在很多例子中 $T(y)=y$ 。这便意味着预测结果 $h(x)$ 满足 $h(x)=E[y \mid x]$ 
3. 自然参数 $\eta$ 与 $x$ 为线性关系，即 $\eta=\theta^Tx$

##### 回归

###### 普通最小二乘

根据 GLM 第一个假设 ，对于给定的 $x$ ，$y \sim N(\mu,\sigma^2)$ ，根据假设二、三，我们知道 
$$
\widehat{y}=h_{\theta}(x)=E[y \mid x;\theta]=\mu=\eta=\theta^Tx
$$

> 正态分布作为指数族分布参数 $\eta$ 即期望为一结论易证

也就是说用最小二乘法估计**线性回归**的参数是需要对于给定的 $x$ 假设 $y$ 服从正态分布

> 事实上，最小二乘法的合理性可以通过极大似然法给出解释：（假设自变量为1维）
>
> 设 $y=\theta^Tx+\varepsilon$ ，而 $\mu=\theta^T x$ ，故而可假设 $\varepsilon \sim N(0,\sigma^2)$ .
>
> 从而似然函数 
> $$
> L(\theta)=\prod_{i=1}^np(y(i)\mid x(i);\theta) \\
> =\prod_{i=1}^n\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(y(i)-\theta^T x(i))^2}{2\sigma^2})
> $$
> 最大化上述似然函数的对数，化简后**等价于**最小化最小二乘法

然而，对于普通最小二乘的系数估计问题，其依赖于模型各项的相互**独立性**。当各项是相关的，且设计矩阵（样本矩阵） $X$ 的各列近似线性相关，那么，设计矩阵会趋向于奇异矩阵，这会导致最小二乘估计对于随机误差非常敏感，产生很大的**方差**。

例如，在没有实验设计的情况下收集到的数据，这种**多重共线性**（multicollinearity）的情况可能真的会出现。 可有一些方法。

###### 岭（Ridge）回归

参考：[python机器学习](http://sklearn.apachecn.org/cn/0.19.0/modules/linear_model.html#ordinary-least-squares)

[`Ridge`](http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge) 回归通过对系数的大小施加惩罚来解决 [普通最小二乘法] 的过拟合和多重共线性问题。 岭系数最小化的是带**罚项**的残差平方和

其模型为
$$
\min \limits_{\boldsymbol{\theta}}\quad L(\boldsymbol{\theta})=\frac{1}{N}\sum_{i=1}^N(y_i-\boldsymbol{\theta}^T\boldsymbol{x}_i)^2+\alpha||\boldsymbol{\theta}||_2^2
$$

> 其中每一个 $x_i$ 均为与 $\theta$ 等长的**向量**

其中 $\alpha \ge 0$ 是控制系数收缩量的复杂性参数：$\alpha$ 的值越大，系数向量越趋近于零，模型越简单，系数对**多重共线性的鲁棒性**越强。

![../_images/sphx_glr_plot_ridge_path_0011.png](http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_ridge_path_0011.png)

 实际中正则化参数 $\alpha$ 通过交叉验证求解，python中可以用 `RidgeCV `求解

```python
>>> from sklearn import linear_model
>>> reg = linear_model.RidgeCV(alphas=[0.1, 1.0, 10.0])
>>> reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])       
RidgeCV(alphas=[0.1, 1.0, 10.0], cv=None, fit_intercept=True, scoring=None,
    normalize=False)
>>> reg.alpha_                                      
0.1
```

该对象与 GridSearchCV 的使用方法相同，只是它默认为 Generalized Cross-Validation(广义交叉验证 GCV)，这是一种有效的**留一验证方法**（LOO-CV）: 

> 看看有没有 $S$ 折交叉验证的参数项，留一交叉验证一般适用于1数据较少时

###### Lasso回归

The [`Lasso`](http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso) 是估计**稀疏系数**的线性模型。 它在一些情况下是有用的，因为它倾向于使用具有较少参数值的情况，有效地**减少**给定解决方案所依赖**变量**的数量。 因此，Lasso 及其变体是压缩感知领域的基础。 在**一定条件**下，它可以**恢复**一组**非零权重**的精确集 
$$
\min \limits_{\boldsymbol{\theta}}\quad L(\boldsymbol{\theta})=\frac{1}{2N}\sum_{i=1}^N(y_i-\boldsymbol{\theta}^T\boldsymbol{x}_i)^2+\alpha||\boldsymbol{\theta}||_1
$$

###### 多项式回归

实际上利用**基函数**可以将多项式回归化为线性回归

![\hat{y}(w, x) = w_0 + w_1 x_1 + w_2 x_2 + w_3 x_1 x_2 + w_4 x_1^2 + w_5 x_2^2](http://sklearn.apachecn.org/cn/0.19.0/_images/math/1e1f74179df321954b823943c08d555a524e69f9.png)

设 $z=[x_1,x_2,x_1x_2,x_1^2,x_2^2]$ ，则

![\hat{y}(w, x) = w_0 + w_1 z_1 + w_2 z_2 + w_3 z_3 + w_4 z_4 + w_5 z_5](http://sklearn.apachecn.org/cn/0.19.0/_images/math/618623438e33ecf053d364a0b938f056cd758b34.png)

##### 分类

###### logistic 回归

logistic 回归，虽然名字里有 “回归” 二字，但实际上是解决分类问题的一类线性模型。

作为一个广义线性模型，对于给定的 $x$ ，$y$ 服从二项分布 $B(\varphi)$ ，并且由上文知
$$
\widehat{y}=E[y\mid x;\theta]=\varphi=\frac{1}{1+e^{-\eta}}=\frac{1}{1+e^{-\theta^Tx}}
$$

> scikit-learn 中 logistic 回归在 [`LogisticRegression`](http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression) 类中实现了二分类（binary）、一对多分类（one-vs-rest）及多项式 logistic 回归，并带有可选的 L1 和 L2 正则化。 
>
> 注意实现后的可视化

###### softmax 回归

> 参考：《机器学习斯坦福公开课》

此处我们的分类结果为 $k$ 类，在二分类中，$\varphi$ 刻画了是否取1的概率，显然多元分类仅仅依靠一个 $\varphi$ 是不够的，我们选取 $k-1$ 个参数来表示取第 $i$ 类的概率，下面将伯努利分布推广至多项的情形：

> 原本应该用 $k$ 个参数，但是概率和为1，所以可以减小一个自由度

为了更好的表示多项指数族分布，定义 $T(y) \in R^{k-1}$ 如下：

![](https://i.loli.net/2018/08/10/5b6d9ed6b8a10.jpg)

由实行函数可以表达为 $\mathbf{1}\{y=i\}=T(y)_i$ ，因此 $E[T(y)_i]=P\{y=i\}=\varphi_i$ 。

由独立事件的乘法法则有
$$
\begin{align}
p(y;\varphi)&=\varphi_1^{\mathbf{1}\{y=1\}}\varphi_2^{\mathbf{1}\{y=2\}}\cdots\varphi_k^{\mathbf{1}\{y=k\} } \\
&=\varphi_1^{\mathbf{1}\{y=1\}}\varphi_2^{\mathbf{1}\{y=2\}}\cdots\varphi_k^{1-\sum \limits_{i=1}^{k-1}T(y) _i} \\
&=b(y)e^{\eta^TT(y)-a(\eta)}   \\
  \end{align}
$$
其中
$$
\begin{align}
\eta&  = \left[ {\begin{array}{*{20}{c}}
{\log ({\varphi _1}/{\varphi _k})}\\
{\log ({\varphi _2}/{\varphi _k})}\\
 \vdots \\
{\log ({\varphi _{k - 1}}/{\varphi _k})}
\end{array}} \right] \\
a(\eta) &=-\log(\varphi_k) \\
b(y)&=1 
\end{align}
$$
由 $\eta$ 表达式可得 $\varphi_i=\varphi_k e^{\eta_i}$ ,由 $\sum \varphi_i=1$ 可得
$$
\varphi_i=\frac{e^{\eta_i}}{\sum \limits_{i=1}^ke^{\eta_i}}
$$
改方程将 $\varphi_i$ 和 $\eta_i=\theta_i^Tx$ 联系了起来，称之为 softmax 函数。
$$
p(y=i \mid x;\theta)=\varphi_i=\frac{\exp(\theta_i^Tx)}{\sum \limits_{i=1}^k\exp(\theta_i^Tx)}
$$
称之为 softmax 回归。同样可以使用最大似然法估计参数。不过这里的参数有 $k$ 个**向量** ，即 $\theta_i,i=1,2,\cdots,k$ 。其中 $\boldsymbol{\theta}_k=\boldsymbol{0}$ 

> 如果类别间相互排斥，可以选择softmax 分类器，如果不互斥，即可以一对多分类，选择 $K$ 个独立的二分类的 logistic 分类器更加合适。编程时看看 scikit-learn 中的一对多分类用的啥方法~

#### 

#### 支持向量机（SVM）

#### 朴素贝叶斯

#### $k$ 近邻

#### 感知机

#### 决策树

#### 线性与二次判别分析

#### 集成方法





#### 标注算法